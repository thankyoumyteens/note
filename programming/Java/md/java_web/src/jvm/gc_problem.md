# 你实际遇到过的 GC 问题有哪些？具体是怎么定位和解决的？

印象比较深的是在 XX 系统里，有一段时间线上出现了 **吞吐量下降、RT(Response Time，响应时间) 偶尔飙高** 的情况，监控里能看到 Full GC 次数明显增多。

定位步骤大致是：

1. 先通过 `jstat` 和监控平台查看 Young GC / Full GC 的频率和停顿时间，确认问题确实集中在 Full GC 上。
2. 导出一段时间的 GC 日志，用 GCViewer / GCeasy 这类工具分析，发现 Old 区回收效果不好，而且晋升速率过快。
3. 再通过 `jmap -histo` 和堆 dump（用 MAT 分析），看到大量的临时对象被频繁创建，集中在某些批量导出和大列表查询的地方，尤其是一些中间 DTO、Map 组装。

解决上主要做了几件事：

- 对大列表查询增限制条件，避免一次性加载过多对象；
- 减少多余的对象拷贝和中间 Map 组装，部分改成流式处理；
- 调整了 JVM 参数，比如适当增大新生代比例，让更多短命对象在新生代就被回收。

优化之后，Full GC 次数明显下降，接口 P99 响应时间也有明显改善。

## 名词解释

### 吞吐量

单位时间内系统处理的请求数。比如：

- QPS（Queries Per Second）：每秒请求数
- TPS（Transactions Per Second）：每秒事务数

### P99 响应时间

在一段统计周期内（比如 1 分钟 / 5 分钟），把这个接口所有请求的响应时间从小到大排个序，有 99% 的请求响应时间都小于等于的那个值，就叫 P99 响应时间。它关注的是 “最慢的那一小撮请求” 的表现，而不是平均值

举个特别直观的例子：

某接口 1 分钟内有 1000 个请求，把 1000 个请求的 RT 排序后，如果排在第 990 个的 RT 是 800ms，那这一分钟，这个接口的 P99 RT = 800ms。

这就表示：这一分钟里，有 99% 的请求在 800ms 以内返回了，只有最后那 1% 比 800ms 慢。

P99 高就意味着：有小部分请求经常被长时间阻塞，例如：

- GC 暂停
- 某个下游接口偶发超时重试
- 某个锁竞争/连接池耗尽

---

## 深入追问

### 怎么确认是 Full GC 导致吞吐量下降和 P99 飙高的？

当时我是结合监控和 GC 指标一起看的。

1. 先在监控平台上看：
   - 那段时间 **Full GC 次数**明显上升；
   - 每次 Full GC 的 **STW 暂停时间**都在几百毫秒甚至接近 1 秒；
   - 把接口的 **P99 响应时间曲线**和 GC 事件时间线对齐之后，发现：
     - P99 每次出现尖刺的时间，都能跟一两次 Full GC 的时间点**一一对应**。
2. 再看应用层面的表现：
   - 业务侧整体 QPS 基本稳定，并没有突增；
   - 但是在 Full GC 的时间点附近，**吞吐量明显掉下去**，同时线程池里有大量请求在等待；
   - CPU 利用率也能看到一个「突然业务 CPU 掉下去，GC 线程占比升高」的现象。

综合这些信息，我们基本可以确定：  
**是 Full GC 的 STW 挂起了业务线程，导致那一小段时间里大量请求被阻塞，从而表现为吞吐量下降和 P99 偶发尖刺。**

---

### 你说 Old 区回收效果不好、晋升速率快，是怎么看出来的？

这个主要是结合 GC 日志和分析工具看的。

1. 老年代回收效果不好：
   - 用 GCViewer / GCeasy 打开那段时间的 gc.log 后：
     - 每次 Full GC 前后，**老年代的占用量几乎不怎么掉**；
     - 比如，GC 前老年代 6.5G，GC 后还有 6.2G，回收比例很低；
   - 工具里的曲线也是：
     - 老年代使用量是一个**持续上升的趋势**，只有很小的锯齿，说明 Full GC 对老年代的清理效果很有限。
2. 晋升速率过快：
   - 观察 **Young GC 之后老年代的占用**：
     - 每次 Young GC 结束后，老年代都会“台阶状”往上涨一截；
   - 在 GC 工具里能看到 **Promotion Rate（晋升速率）**这个指标明显偏高：
     - 新生代每次 GC 完，都会有大量对象被晋升到老年代；
     - 导致老年代很快被填满，引发频繁的 Full GC。

综合来看，就是：  
**短命对象本来应该在年轻代多经历几次 Young GC 就被清掉，但因为某些场景下瞬时对象太多，直接把新生代挤爆，导致大量对象被被动晋升到老年代，老年代很快占满，但里面很多其实是“伪长寿”的对象，回收效果就很差。**

---

### 你提到用 `jmap`、堆 dump，具体是怎么定位到“临时对象很多”的？

这里我分两步做的：

1. 在线用 `jmap -histo` 做一个抽样：
   - 在问题比较明显的时段，用 `jmap -histo` 看一下对象分布：
     - 能看到有些类的实例数量非常多，比如某些 DTO、List、Map 相关的对象；
     - 而且这些对象的大小加起来占了很大一块堆。
   - 这一步可以先大致定位「哪些类」占用比较多。
2. 再抓一次堆 dump，用 MAT 做详细分析：
   - 把 dump 丢到 MAT 里：
     - 看 **Dominator Tree**，确认是哪些对象在“主导”内存占用；
     - 分析这些对象的引用链，发现它们主要集中在：
       - 某些批量导出功能；
       - 以及大列表查询 + 复杂 Map / DTO 组装的那几块逻辑。
   - 很多都是**中间对象**，用完就扔的那种，比如：
     - 中间 DTO；
     - 大量 `List<Map<String, Object>>`；
     - 还有多次转换过程中的临时集合。

这么一串下来，基本能确认：  
**是这些批处理和大查询场景在短时间内创建了大量临时对象，挤爆了新生代，推高了老年代晋升和 Full GC 频率。**

---

### 你做了哪些优化？为什么这么做能解决问题？

1. 减少一次性加载的大对象数量：
   - 对大列表查询减小了每页的条数，避免一次性查出特别多的数据进内存；
   - 对一些批量导出，改成**分批处理**或者流式写出；
   - 这样做的目的是：
     - 把原来「一次性在堆里堆一大坨对象」的行为，拆成多次小批量操作；
     - 高峰期瞬时内存占用就会下降，新生代/老年代的压力自然小很多。
2. 减少中间对象和重复拷贝：
   - 优化中间 DTO、Map 组装过程：
     - 能复用的结构尽量复用；
     - 能直接流式写出去的就不存一整个大 List 在内存里；
   - 这些改动本质上是**降低短命对象的生成量**：
     - 短命对象少了，新生代不会那么快装满；
     - 晋升到老年代的对象自然也会减少。
3. 配合调整 JVM 参数：
   - 适当 **增大新生代比例**，给短命对象更多的“缓冲区”：
     - 让它们有机会在 Young GC 中直接被回收，而不是容易被挤到老年代；

最终的效果是：

- Full GC 频率明显下降；
- 老年代使用曲线更加平稳，不再一路上窜；
- 接口的 P99 响应时间，在业务高峰期也从之前的偶发几秒，降回到了正常的几百毫秒以内。

---

### 那如果下次遇到 Full GC 频繁，但堆看起来并没有被塞满，或者对象增长没那么夸张，你会从哪些方向排查？

我一般会先把可能性按几大类排一下：

1. 内存参数设置不合理：
   - 总堆太小、老年代太小，稍微有点突发流量就顶不住；
   - 或者 MaxMetaspace 太小，类加载频繁导致 Metaspace 触发 Full GC。
2. 内存泄漏 / 引用没释放：
   - 某些缓存、静态集合、线程本地变量（ThreadLocal）里一直累积对象；
   - MAT 里看 Dominator Tree 或者 Leak Suspects Report，找到这些长链条引用。
3. 频繁的 System.gc() 或类似行为：
   - 代码里显式调用了 `System.gc()`，或者用了某些三方库/老监控工具；
   - 这些会强制触发 Full GC，得通过 grep 代码、监控或 JVM 参数关闭/限制。
4. 大量大对象直接进老年代：
   - 比如频繁分配几 MB 级别的数组或 ByteBuffer；
   - 可能触发了大对象直接进入老年代（取决于 GC 算法和配置）。

排查思路还是那套：  
**先用监控/GC 日志缩小范围 -> 用堆分析工具确认对象分布和引用链 -> 对症下药调代码和 JVM 参数。**
